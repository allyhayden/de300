{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3 - Ally Hayden"
      ],
      "metadata": {
        "id": "20ZuF_e8Yq50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TF-IDF Definition"
      ],
      "metadata": {
        "id": "O5nQsxTDYtNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the dataset"
      ],
      "metadata": {
        "id": "rDki-jQ9dmqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01uft9VAYmR_",
        "outputId": "6752dfa3-054e-4db2-9fa4-2e6f7183e212"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0  22.9M      0  0:00:01  0:00:01 --:--:-- 22.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YWn-YtTWXaxZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9YHqXfhaI9q",
        "outputId": "7ce6f7f2-84ea-4a66-8516-c5f53e87a02e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "|_c0|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explode the words (make each word a separate row)"
      ],
      "metadata": {
        "id": "mvW-DhhBdpUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "tokens = agnews.select(col(\"_c0\").alias(\"id\"), explode(col(\"filtered\")).alias(\"term\"))\n",
        "tokens.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mWbriYGcN8w",
        "outputId": "1aaa253b-4885-466d-c10b-f6bc31f41fa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id| term|\n",
            "+---+-----+\n",
            "|  0| wall|\n",
            "|  0|   st|\n",
            "|  0|bears|\n",
            "|  0| claw|\n",
            "|  0| back|\n",
            "+---+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Term Frequencies per Document (Numerator)"
      ],
      "metadata": {
        "id": "P0NMmIMUdyJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "tf_counts = tokens.groupBy(\"id\", \"term\").count().withColumnRenamed(\"count\", \"tf_raw\")\n",
        "tf_counts.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq-Gi-RRcUGW",
        "outputId": "b1087152-2788-477c-8627-7909ff490fe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "| id|      term|tf_raw|\n",
            "+---+----------+------+\n",
            "| 10|    stocks|     1|\n",
            "| 21|    nation|     1|\n",
            "| 36|      news|     2|\n",
            "| 44|     salem|     1|\n",
            "| 48|government|     1|\n",
            "+---+----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Total Terms per Document (Denominator)"
      ],
      "metadata": {
        "id": "RoJ1vDmHd2y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lengths = tokens.groupBy(\"id\").count().withColumnRenamed(\"count\", \"total_terms\")\n",
        "doc_lengths.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Js5shGc1H4",
        "outputId": "90a77972-e867-413c-9ff8-daf06015bac4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+\n",
            "| id|total_terms|\n",
            "+---+-----------+\n",
            "|148|         32|\n",
            "|463|         24|\n",
            "|471|         24|\n",
            "|496|         31|\n",
            "|833|         13|\n",
            "+---+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Normalized TF"
      ],
      "metadata": {
        "id": "sitntGgFd5Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "tf = tf_counts.join(doc_lengths, on=\"id\")\n",
        "tf = tf.withColumn(\"tf\", col(\"tf_raw\") / col(\"total_terms\"))\n",
        "tf.select(\"id\", \"term\", \"tf\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbuMPveFc9bC",
        "outputId": "cbde3a3c-eb36-43f0-e11d-c2f727c92c73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+--------------------+\n",
            "|  id|     term|                  tf|\n",
            "+----+---------+--------------------+\n",
            "| 833|      hit| 0.07692307692307693|\n",
            "|1088|following| 0.05555555555555555|\n",
            "|1959|      ups| 0.06896551724137931|\n",
            "|1959|    block|0.034482758620689655|\n",
            "|6397|   friday|0.037037037037037035|\n",
            "+----+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "iRx-pPeid7tW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count document frequency\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "df_counts = tokens.select(\"id\", \"term\").distinct().groupBy(\"term\").agg(countDistinct(\"id\").alias(\"df\"))\n",
        "df_counts.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZBU2M2fdCTe",
        "outputId": "dbf7c78b-f378-4d79-8f91-fa2daedbe44a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+\n",
            "|         term|  df|\n",
            "+-------------+----+\n",
            "|        still|2281|\n",
            "|      acidity|   2|\n",
            "|       online|2444|\n",
            "|precautionary|   6|\n",
            "|       harder|  82|\n",
            "+-------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get total number of documents\n",
        "num_docs = agnews.select(\"_c0\").distinct().count()\n",
        "print(\"Total documents:\", num_docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGsDqsmodcMq",
        "outputId": "51500458-9c8e-4bda-994f-b3977a3a0041"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 127600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# idf = log(N / df)\n",
        "from pyspark.sql.functions import log, lit\n",
        "\n",
        "idf = df_counts.withColumn(\"idf\", log(lit(num_docs) / col(\"df\")))\n",
        "idf.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtPMk1URdfw9",
        "outputId": "da87af88-aef8-4c0c-97c9-6bdd8b6cd007"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+------------------+\n",
            "|         term|  df|               idf|\n",
            "+-------------+----+------------------+\n",
            "|        still|2281|4.0242864276084385|\n",
            "|      acidity|   2| 11.06350846933288|\n",
            "|       online|2444|3.9552643296013406|\n",
            "|precautionary|   6| 9.964896180664772|\n",
            "|       harder|  82| 7.349936402628574|\n",
            "+-------------+----+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute TF-IDF by joining TF and IDF"
      ],
      "metadata": {
        "id": "jhTiKyMteS6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = tf.join(idf, on=\"term\")\n",
        "tfidf = tfidf.withColumn(\"tfidf\", col(\"tf\") * col(\"idf\"))\n",
        "\n",
        "# filter for first 5 documents\n",
        "tfidf.filter(col(\"id\") < 5).select(\"id\", \"term\", \"tfidf\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy_sc6wzdfpW",
        "outputId": "9ccb5b3d-86ab-49cf-d9cc-a19c9e6cfb67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-------------------+\n",
            "|id |term       |tfidf              |\n",
            "+---+-----------+-------------------+\n",
            "|4  |posing     |0.2589223867776184 |\n",
            "|3  |pipeline   |0.4720829409342409 |\n",
            "|0  |wall       |0.5115985326511431 |\n",
            "|2  |doldrums   |0.3770252270329423 |\n",
            "|2  |stocks     |0.14976769101715193|\n",
            "|4  |us         |0.1669859687392097 |\n",
            "|4  |present    |0.22209684830286883|\n",
            "|2  |stock      |0.17879168082328206|\n",
            "|3  |exports    |0.2146590164054526 |\n",
            "|1  |industry   |0.15043731768548949|\n",
            "|3  |iraq       |0.23809526243476142|\n",
            "|1  |aerospace  |0.2581171817448437 |\n",
            "|2  |prices     |0.14472559202114177|\n",
            "|4  |prices     |0.23156094723382684|\n",
            "|1  |toward     |0.1898997183872362 |\n",
            "|0  |cynics     |0.563734318747707  |\n",
            "|3  |authorities|0.18159366801541998|\n",
            "|4  |menace     |0.5747440955975784 |\n",
            "|1  |carlyle    |0.7168306746824437 |\n",
            "|4  |records    |0.19759033440942064|\n",
            "+---+-----------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) Group TF-IDF values back per document"
      ],
      "metadata": {
        "id": "028LQlrQeTa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import struct, collect_list\n",
        "\n",
        "# Group TF-IDF scores into a list per document\n",
        "grouped = tfidf.select(\"id\", \"term\", \"tfidf\") \\\n",
        "               .withColumn(\"term_tfidf\", struct(\"term\", \"tfidf\")) \\\n",
        "               .groupBy(\"id\").agg(collect_list(\"term_tfidf\").alias(\"tfidf_scores\"))\n",
        "\n",
        "grouped.show(5, truncate=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxiBBjPpdfhn",
        "outputId": "b9cde351-b89a-4c9b-fdc4-e8150a9458f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------------------------------------------------------------------------------+\n",
            "| id|                                                                                        tfidf_scores|\n",
            "+---+----------------------------------------------------------------------------------------------------+\n",
            "|  1|[{investment, 0.1890771769001148}, {commercial, 0.2057832028092643}, {reputation, 0.2578098186776...|\n",
            "|  3|[{exports, 0.2146590164054526}, {infrastructure, 0.22959926718225876}, {reuters, 0.15913296762843...|\n",
            "|  5|[{positive, 0.18127557126337487}, {46, 0.2067185029184427}, {o, 0.1405921241478995}, {end, 0.1131...|\n",
            "|  6|[{98, 0.24380014644675033}, {billion, 0.12463394966614495}, {money, 0.32032556569959436}, {thursd...|\n",
            "|  9|[{wall, 0.48467229409055657}, {green, 0.27256812064061997}, {ultra, 0.3908380162950787}, {new, 0....|\n",
            "+---+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. SVM Objective Function"
      ],
      "metadata": {
        "id": "WG7USsVQoIR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the dataset (no header)"
      ],
      "metadata": {
        "id": "LjxatbxHoM6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qnRwFuwdfYK",
        "outputId": "fc305192-8a7e-4237-ebab-b2748f3af333"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0  12232      0 --:--:-- --:--:-- --:--:-- 12309\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0    243      0 --:--:-- --:--:-- --:--:--   244\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  25.5M      0  0:00:02  0:00:02 --:--:-- 25.5M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, DoubleType\n",
        "\n",
        "svm_schema = StructType(\n",
        "    [StructField(f\"f{i}\", DoubleType(), True) for i in range(64)] +\n",
        "    [StructField(\"label\", DoubleType(), True)]\n",
        ")\n",
        "\n",
        "svm_df = spark.read.csv(\"data_for_svm.csv\", schema=svm_schema, header=False)\n",
        "\n",
        "svm_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF-304SZoZuh",
        "outputId": "8f37cf01-bac5-4549-f167-a5144ae80fc5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---+----+----+----+----+----+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+\n",
            "|  f0|  f1|  f2|  f3|  f4|  f5|  f6|  f7|  f8|  f9| f10| f11| f12| f13| f14|f15| f16| f17| f18| f19| f20| f21| f22| f23|f24| f25| f26| f27| f28| f29| f30| f31| f32| f33| f34| f35| f36| f37| f38| f39| f40| f41| f42| f43| f44| f45| f46| f47| f48| f49| f50| f51| f52| f53| f54| f55| f56| f57| f58| f59| f60| f61| f62| f63|label|\n",
            "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---+----+----+----+----+----+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+\n",
            "|-1.0|-1.0| 1.0| 1.0|-1.0| 1.0| 1.0| 1.0| 1.0|-1.0| 1.0| 1.0|-1.0|-1.0|-1.0|1.0|-1.0| 1.0|-1.0| 1.0|-1.0| 1.0| 1.0|-1.0|1.0| 1.0| 1.0|-1.0| 1.0|-1.0|-1.0|-1.0| 1.0| 1.0| 1.0|-1.0| 1.0|-1.0|-1.0|-1.0|-1.0| 1.0|-1.0| 1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0| 1.0| -1.0|\n",
            "| 1.0| 1.0|-1.0| 1.0|-1.0|-1.0| 1.0| 1.0|-1.0| 1.0| 1.0|-1.0| 1.0|-1.0| 1.0|1.0| 1.0| 1.0|-1.0| 1.0|-1.0|-1.0| 1.0|-1.0|1.0|-1.0|-1.0| 1.0| 1.0| 1.0| 1.0|-1.0|-1.0| 1.0| 1.0|-1.0|-1.0|-1.0|-1.0| 1.0| 1.0| 1.0| 1.0| 1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0| 1.0| 1.0|-1.0|  1.0|\n",
            "| 1.0| 1.0| 1.0|-1.0| 1.0| 1.0|-1.0| 1.0|-1.0| 1.0|-1.0|-1.0| 1.0|-1.0|-1.0|1.0|-1.0|-1.0| 1.0|-1.0|-1.0|-1.0| 1.0|-1.0|1.0| 1.0|-1.0|-1.0| 1.0| 1.0|-1.0|-1.0| 1.0| 1.0|-1.0| 1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0| 1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0| 1.0| 1.0| 1.0|  1.0|\n",
            "| 1.0|-1.0| 1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0| 1.0|-1.0| 1.0| 1.0| 1.0|-1.0|1.0| 1.0| 1.0|-1.0| 1.0| 1.0| 1.0|-1.0| 1.0|1.0|-1.0| 1.0|-1.0| 1.0|-1.0| 1.0| 1.0|-1.0|-1.0|-1.0|-1.0|-1.0|-1.0| 1.0|-1.0|-1.0|-1.0|-1.0| 1.0|-1.0|-1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0|  1.0|\n",
            "| 1.0|-1.0| 1.0|-1.0| 1.0|-1.0|-1.0| 1.0| 1.0| 1.0| 1.0| 1.0|-1.0|-1.0|-1.0|1.0|-1.0| 1.0| 1.0| 1.0|-1.0| 1.0|-1.0| 1.0|1.0|-1.0| 1.0| 1.0|-1.0|-1.0|-1.0| 1.0|-1.0| 1.0|-1.0|-1.0| 1.0|-1.0| 1.0| 1.0| 1.0|-1.0| 1.0|-1.0| 1.0|-1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| -1.0|\n",
            "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---+----+----+----+----+----+----+----+----+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Weights and Bias"
      ],
      "metadata": {
        "id": "o87wvLAhp4XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "w = pd.read_csv(\"w.csv\", header=None).values.flatten()\n",
        "bias = pd.read_csv(\"bias.csv\", header=None).values[0][0]\n",
        "\n",
        "print(\"w shape:\", w.shape)   # should be (64,)\n",
        "print(\"bias:\", bias)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DimOnj9wpTGL",
        "outputId": "cfca290e-4822-4ba9-cd54-dec36e79ee29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w shape: (64,)\n",
            "bias: 0.0001495661647902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Hinge Loss per Row"
      ],
      "metadata": {
        "id": "KJawxvvvp9Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining/applying UDF\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "w_broadcast = spark.sparkContext.broadcast(w)\n",
        "bias_broadcast = spark.sparkContext.broadcast(bias)\n",
        "\n",
        "# hing loss function\n",
        "def compute_hinge_loss(*row):\n",
        "    x = np.array(row[:-1])   # all 64 features\n",
        "    y = row[-1]              # the label\n",
        "    margin = y * (np.dot(w_broadcast.value, x) + bias_broadcast.value)\n",
        "    return float(max(0.0, 1 - margin))\n",
        "\n",
        "hinge_udf = udf(compute_hinge_loss, DoubleType())\n"
      ],
      "metadata": {
        "id": "Zu4Yx5_qpdjI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying UDF to the dataframe\n",
        "columns = [f\"f{i}\" for i in range(64)] + [\"label\"]\n",
        "\n",
        "svm_df = svm_df.withColumn(\"hinge_loss\", hinge_udf(*[col(c) for c in columns]))\n",
        "\n",
        "svm_df.select(\"hinge_loss\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQa5RPS2qM3A",
        "outputId": "7b738a03-27a1-4dbc-d650-4cef3cdd8eaa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|        hinge_loss|\n",
            "+------------------+\n",
            "|0.9493088124624688|\n",
            "|1.0629362913461953|\n",
            "|1.0707972344482695|\n",
            "|0.9978622601633201|\n",
            "|0.9916031666951637|\n",
            "+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute SVM Objective Function"
      ],
      "metadata": {
        "id": "RNfazMjfqVbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute avg hinge loss\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "avg_hinge_loss = svm_df.select(avg(\"hinge_loss\")).first()[0]\n",
        "print(\"Average hinge loss:\", avg_hinge_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cjJPM5NqNSm",
        "outputId": "77ab8852-2228-4a95-e71f-0ed5bee32a5e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average hinge loss: 0.9997237624117761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# regularization\n",
        "λ = 0.1\n",
        "\n",
        "l2_norm_squared = float(np.dot(w, w))\n",
        "reg_term = λ * l2_norm_squared\n",
        "print(\"Regularization term:\", reg_term)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGXCWAwyqXyO",
        "outputId": "30a505da-066c-4c24-aeba-9d5dda898d6b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularization term: 0.00032166210739758135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final loss result\n",
        "svm_loss = reg_term + avg_hinge_loss\n",
        "print(\"Final SVM Loss:\", svm_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1e7uCxqqY5r",
        "outputId": "1c2eefd3-22fc-4975-c2ff-aa0eacb82965"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final SVM Loss: 1.0000454245191737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Disclosure"
      ],
      "metadata": {
        "id": "7ekRWRvPa8D1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions asked to ChatGPT:\n",
        "\n",
        "- TF-IDF Section\n",
        " - Why am I getting an 'unresolved column' error when I reference 'id' in my DataFrame?\n",
        " - How to use the .filter() function in PySpark.\n",
        "\n",
        "- SVM Loss Section\n",
        " - Why is my UDF for hinge loss giving me a type error?\n",
        " - How should I choose the lambda value for SVM regularization?"
      ],
      "metadata": {
        "id": "VCMAAwMqa-ww"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlObywNFqvtc"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}